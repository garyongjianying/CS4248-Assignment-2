{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fee039",
   "metadata": {},
   "source": [
    "# Use case: Put the jupyter notebook in the location together with train.csv and test.csv and run the entire notebook. The A0155664X.csv file will be generated in the same location as the final output predictions from the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c309774",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gary2\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Gary2\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\Gary2\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import keras\n",
    "\n",
    "# Imblearn\n",
    "# pip install imblearn\n",
    "# !pip3 install imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# the following installations are required\n",
    "# !pip3 install textblob\n",
    "# !pip3 install spacytextblob\n",
    "# !pip3 install textblob\n",
    "# !pip3 install functools\n",
    "# !pip3 install keras\n",
    "\n",
    "# python -m pip install textblob\n",
    "# python -m textblob.download_corpora\n",
    "# python -m spacy download en_core_web_sm\n",
    "# pip install spacytextblob\n",
    "# pip install -U textblob\n",
    "# python -m textblob.download_corpora\n",
    "# pip install nlpaug\n",
    "# from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "# from textblob import TextBlob\n",
    "# from functools import reduce\n",
    "# import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "randomseed=1\n",
    "from numpy.random import seed\n",
    "seed(randomseed)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(randomseed)\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, make_scorer, roc_auc_score, plot_roc_curve, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score # K-fold cross validation to be performed for hyperparameter tuning\n",
    "# import libraries required for lemmatization and for stopwords removal\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# import models required\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression\n",
    "from sklearn.naive_bayes import MultinomialNB #  is suitable for classification with discrete features (e.g., word counts for text classification).\n",
    "\n",
    "# Simple NN\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy, BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "\n",
    "# Setting filename for output csv file\n",
    "filename = 'A0155664X.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a0961",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6572e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Count/Percentage of each class of statements')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv') # df forms our training dataset\n",
    "\n",
    "non_fact_statements = df.loc[df['Verdict'] == -1]\n",
    "unimportant_statements = df.loc[df['Verdict'] == 0]\n",
    "fact_statements = df.loc[df['Verdict'] == 1]\n",
    "\n",
    "# Percentage of each class\n",
    "non_fact_percent = round(len(non_fact_statements)/len(df)*100,2)\n",
    "unimportant_percent = round(len(unimportant_statements)/len(df)*100,2)\n",
    "fact_percent = round(len(fact_statements)/len(df)*100,2)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "sns.countplot(x='Verdict', data=df)\n",
    "plt.annotate(f'Non-facts: {len(non_fact_statements)}', \n",
    "             xy=(-0.3, 3000), # xy = (x dist from 0, y dist from 0)\n",
    "            size=12)\n",
    "\n",
    "plt.annotate(f'Unimportant: {len(unimportant_statements)}', \n",
    "             xy=(0.68, 1250), # xy = (x dist from 0, y dist from 0)\n",
    "            size=12)\n",
    "plt.annotate(f'Facts: {len(fact_statements)}', \n",
    "             xy=(1.8, 3000), # xy = (x dist from 0, y dist from 0)\n",
    "            size=12)\n",
    "\n",
    "\n",
    "plt.annotate(f'{non_fact_percent}%', xy=(-0.2, 5000),size=12)\n",
    "plt.annotate(f'{fact_percent}%', xy=(1.9, 4000),size=12)\n",
    "plt.annotate(f'{unimportant_percent}%', xy=(0.85, 1800),size=12)\n",
    "ax.set_title('Count/Percentage of each class of statements')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45022c5",
   "metadata": {},
   "source": [
    "###  From the graph countplot shown above, we can see that there is class imbalance, whereby 65.26% of the training data belonging to label -1, and 24.06% in label 1 and finally, 10.68% in label 0.\n",
    "\n",
    "- We will look towards using removal of duplicates and data with contradictory labels to clip to the minority class to try to increase the generalization of the baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b523a",
   "metadata": {},
   "source": [
    "# Summary of Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be192931",
   "metadata": {},
   "source": [
    "- Based on testing F1 score on kaggle with baseline Logistic Regression / Naive Bayes MultinomialNB model, we try to see if our efforts at preprocessing can help improve baseline score.\n",
    "- TfidfVectorizer and CountVectorizer with ngram_range = (1,1) was used to check the results\n",
    "\n",
    "Preprocessing tried:\n",
    "- Casefolding & MinMaxScaler() for data normalization **(good)** - helped with improving LogisticRegression (tfidf) from f1 of 0.79862 to 0.8153\n",
    "- Removal of duplicates and clipping data with contradicting labels to minority class of 0 **(good)** - helped with model generalization of LogisticRegression (tfidf) from 0.8153 to 0.82531, highest baseline score from just preprocessing.\n",
    "- Lemmatization (with POS tagging)\n",
    "- Removal of stopwords\n",
    "- Removal of punctuations\n",
    "\n",
    "For lemmatization, removal of stopwords and punctuations, they were bad for model generalization as they led to decreased f1 score across the board, and hence will not be pursued for the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d748c4",
   "metadata": {},
   "source": [
    "### In summary, the best baseline model score resulting from preprocessing ended up at 0.82531."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea746e",
   "metadata": {},
   "source": [
    "# Summary of Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cc282",
   "metadata": {},
   "source": [
    "We perform feature engineering on both the training and test datasets to ensure that the model have the same treatment when predicting the labels in the test set. In addition, we must use a MinMaxScaler() for data normalization to ensure that there are no scale-dominant feature during model training that could lead to bias.\n",
    "\n",
    "The feature engineering was performed on the best model previously of LogisticRegression at 0.82531 from preprocessing.\n",
    "\n",
    "Feature Enginering tried:\n",
    "- Text Subjectivity\n",
    "- Text Polarity\n",
    "- Counts of different types of POS Tags by a custom defined function **(good) --> improved test f1 score from 0.82531 to 0.84159.**\n",
    "- Dialog & narrative parser by custom defined function (good)\n",
    "- Unique words in sentence ratio **(good) --> improve test f1 score from 0.84159 to 0.85446**\n",
    "- Stop words in sentence ratio **(good) -->  improve test f1 score from 0.84159 to 0.85446**\n",
    "- Counts of punctuaton within the sentence\n",
    "\n",
    "\n",
    "\n",
    "**Of which, only POS tagging, dialog & narrative parsing and unique/stopwords ratio for each sentence had helped to boost the overall performance of baseline Logistic Regression model from a score of 0.82531 to a score of 0.85446, representing a ~2.9% increase in test f1_score.** Hence, moving forward with hyper-parameter tuning, we will use datasets and pipelines that make adjustments to both the training and test datasets of interest before running different parameters for our models to get the best ones.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed2d05",
   "metadata": {},
   "source": [
    "# Summary of cross-validation & hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9c0",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning was performed for the following models:\n",
    "1. Naive Bayes Model (e.g alpha of NB model)\n",
    "2. Logistic Regression Model (e.g regularization constant, type of regularization penalty)\n",
    "3. Choice of TfidfVectorizer and CountVectorizer Parameters (e.g ngram_range)\n",
    "\n",
    "For this task, we will need to build data pipelines for our hyperparameter tuning.\n",
    "\n",
    "### Model Selection and Usage of Pipeline (IMBLEARN):\n",
    "\n",
    "<u>Train Test Split</u>\n",
    "- stratified train/test split as peformed to maintain similar target variable distribution in both train & test set that is required for cross-validation within the pipeline.\n",
    "- a 15% test split was selected by default to check validation loss/f1_score.\n",
    "\n",
    "<u>Preprocess both training and test datasets for fairness</u>\n",
    "- Based on our best performing model, we will perform the following on initial data in this sequence - **Preprocess: remove duplicates & data with contradicting labels -> Feature Engineering: POS tagging -> dialog & narrative parser -> unique/stopwords ratio -> followed by MinMaxScaler() for data normalization.**\n",
    "\n",
    "<u>Hyper-parameter tuning:</u>\n",
    "- GridSearchCV is used to tune various model parameters, we will use a 15% validation split.\n",
    "- Models used will include: Naive Bayes, Logistic Regression.\n",
    "- 5-fold cross validation will be used to search for the best parameters for each model.\n",
    "\n",
    "<u>Evaluation Metric:</u>\n",
    "- The trained model should have a high cross-validated high f1 score.\n",
    "- By looking at the data from exploratory data analysis (EDA) previously, there is only a slight degree of imbalance in the dataset. Hence, the micro-average/weighted-macro can be used as a suitable scoring metric (credit: https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/24051#24051)\n",
    "\n",
    "\n",
    "<u>Sampling Methods:</u>\n",
    "- We can see that here is some imbalance in the training data, namely minority in unimportant/non-factual statements. Hence, we make use of OverSampler to see if it can improve metric scoring for the models in cross-validation, and eventually on the test dataset on kaggle.\n",
    "\n",
    "## We specify a ColumnTransformer that will be utilized within the pipeline to perform the following:\n",
    "\n",
    "1. TfidfVectorizer / CountVectorizer within our pipeline, only on the 'Text' training column\n",
    "2. MinMaxScaler() across all columns in the columns after tfidfvectorizer/countvectorizer is used.\n",
    "\n",
    "\n",
    "## Define RandomOverSampler that could be used for not majority class.\n",
    "- We will utilize only one form of resampling strategy to see if it helps our predictions on the unseen test data set and also on the cross-validated f1-score.\n",
    "- Oversampling is the choice of selection and it is a technique used to handle class imbalance, which occurs when the number of examples in one class if significantly higher/lower than the number of examples in other classes.\n",
    "- It involves increasing the number of examples in the minority class to balance the class distribution, by duplicating existing examples. SMOTE could also have been used, but it might not be the best, as we can further utilize libraries such as nlpaug which can help to augment better structured sentences with the help of large language models.\n",
    "- For the purpose of this project, we will simply duplicate samples from the minority class to fulfil our purpose.\n",
    "\n",
    "## Some benefits could include:\n",
    "- Improved model performance by reducing bias to majority class\n",
    "- Reduced overfitting from memorizing th training data.\n",
    "- Better generalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33ee4a",
   "metadata": {},
   "source": [
    "# Building a basic multilayer perception\n",
    "- We will utilize a total of 3 hidden layers (256,128,64 units respectively), 1 input & output layer (3 units).\n",
    "- Since it is a multiclass problem, we utilize softmax as the activation for the output layer. Relu activation is utilized for the rest of the hidden layers.\n",
    "- We utilize l2 regularization to prevent overfitting by specifying a kernel regularizer at each layer. We search for the best regularization constant to be used in the GridSearchCV\n",
    "- KerasClassifer is used as a wrapper for the Sequential model so that it can be used as an estimator and to provides a unified interface for use with GridSearchCV.\n",
    "- We utilize BatchNormalization layers in between hidden layers to prevent the problem of vanishing gradients and exploding gradients during backpropagation, which can help to speed up training due to similar data scales and improve generalization by reducing overfitting.\n",
    "- We monitor the validation loss of the neural network training to ensure that there are no overfitting to the training data. To handle this, we utilize EarlyStopping callback with patience of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b4714",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c9d034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>15% Validation Set F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (countvec, no resampling)</td>\n",
       "      <td>{'columntransform__vect__analyzer': 'word', 'columntransform__vect__ngram_range': (1, 2), 'lr__C': 0.6000000000000001, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.774174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression (tfidf, no resampling)</td>\n",
       "      <td>{'columntransform__vect__ngram_range': (1, 1), 'columntransform__vect__norm': 'l2', 'lr__C': 1.0000000000000002, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.763463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (countvec, oversample)</td>\n",
       "      <td>{'columntransform__vect__ngram_range': (1, 2), 'lr__C': 0.6000000000000001, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.763166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes (countvec, no resampling)</td>\n",
       "      <td>{'columntransform__vect__analyzer': 'word', 'columntransform__vect__ngram_range': (1, 2), 'nb__alpha': 0.4}</td>\n",
       "      <td>0.758405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes (tfidf, no resampling)</td>\n",
       "      <td>{'columntransform__vect__ngram_range': (1, 1), 'columntransform__vect__norm': 'l2', 'nb__alpha': 0.2}</td>\n",
       "      <td>0.747992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression (tfidf, oversample)</td>\n",
       "      <td>{'columntransform__vect__ngram_range': (1, 2), 'columntransform__vect__norm': 'l2', 'lr__C': 1.0000000000000002, 'lr__penalty': 'l2'}</td>\n",
       "      <td>0.741148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes (countvec, oversample)</td>\n",
       "      <td>{'columntransform__vect__analyzer': 'word', 'columntransform__vect__ngram_range': (1, 2), 'nb__alpha': 0.2}</td>\n",
       "      <td>0.719429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes (tfidf, oversample)</td>\n",
       "      <td>{'columntransform__vect__ngram_range': (1, 2), 'columntransform__vect__norm': 'l2', 'nb__alpha': 0.2}</td>\n",
       "      <td>0.718536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultdict_copy = {'Logistic Regression (tfidf, no resampling)': [{'columntransform__vect__ngram_range': (1,\n",
    "    1),\n",
    "   'columntransform__vect__norm': 'l2',\n",
    "   'lr__C': 1.0000000000000002,\n",
    "   'lr__penalty': 'l2'},\n",
    "  0.7634632549836359],\n",
    " 'Logistic Regression (tfidf, oversample)': [{'columntransform__vect__ngram_range': (1,\n",
    "    2),\n",
    "   'columntransform__vect__norm': 'l2',\n",
    "   'lr__C': 1.0000000000000002,\n",
    "   'lr__penalty': 'l2'},\n",
    "  0.7411484677179411],\n",
    " 'Logistic Regression (countvec, no resampling)': [{'columntransform__vect__analyzer': 'word',\n",
    "   'columntransform__vect__ngram_range': (1, 2),\n",
    "   'lr__C': 0.6000000000000001,\n",
    "   'lr__penalty': 'l2'},\n",
    "  0.7741743528711693],\n",
    " 'Logistic Regression (countvec, oversample)': [{'columntransform__vect__ngram_range': (1,\n",
    "    2),\n",
    "   'lr__C': 0.6000000000000001,\n",
    "   'lr__penalty': 'l2'},\n",
    "  0.76316572448676],\n",
    " 'Naive Bayes (tfidf, no resampling)': [{'columntransform__vect__ngram_range': (1,\n",
    "    1),\n",
    "   'columntransform__vect__norm': 'l2',\n",
    "   'nb__alpha': 0.2},\n",
    "  0.7479916691460875],\n",
    " 'Naive Bayes (tfidf, oversample)': [{'columntransform__vect__ngram_range': (1,\n",
    "    2),\n",
    "   'columntransform__vect__norm': 'l2',\n",
    "   'nb__alpha': 0.2},\n",
    "  0.7185361499553704],\n",
    " 'Naive Bayes (countvec, no resampling)': [{'columntransform__vect__analyzer': 'word',\n",
    "   'columntransform__vect__ngram_range': (1, 2),\n",
    "   'nb__alpha': 0.4},\n",
    "  0.758405236536745],\n",
    " 'Naive Bayes (countvec, oversample)': [{'columntransform__vect__analyzer': 'word',\n",
    "   'columntransform__vect__ngram_range': (1, 2),\n",
    "   'nb__alpha': 0.2},\n",
    "  0.7194287414459983]}\n",
    "\n",
    "sortedresults = sorted([[k,v[0],v[1]] for k, v in resultdict_copy.items()], key = lambda x: x[2], reverse = True)\n",
    "res = pd.DataFrame(sortedresults, columns = ['Model','Best Params','15% Validation Set F1 Score'])\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(res.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736eaa6",
   "metadata": {},
   "source": [
    "### Using the top 4 models of cross-validation F1-score, we then use those parameters and train a new model on the full training set, and check the respective test f1_score on kaggle.\n",
    "\n",
    "- Results show that the Logistic Regression (tfidf) with no resampling performed the best out of all the models, and hence will be used to output the results into excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3918a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gary2\\anaconda3\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#loading the english language small model of spacy and retrieve the stopwords\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062ab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that generally takes in test dataframe and generates the verdict column in a new .csv file\n",
    "def generate_result(test, y_pred, filename):\n",
    "    \"\"\"generate csv file based on the y_pred\"\"\"\n",
    "    # test is already the dataframe that will open the file\n",
    "    test['Verdict'] = pd.Series(y_pred)\n",
    "    test_output = test.drop(columns=['Text']) # drop the Text column\n",
    "    # Our final output file should have sentence_id, and verdict only\n",
    "    test_output.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe2773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "      <th>num_dialog</th>\n",
       "      <th>num_narrative</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>i think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>for a while, as i recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>so we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>we got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict  \\\n",
       "0            1      i think we've seen a deterioration of values.       -1   \n",
       "1            2  i think for a while as a nation we condoned th...       -1   \n",
       "2            3  for a while, as i recall, it even seems to me ...       -1   \n",
       "3            4  so we've seen a deterioration in values, and o...       -1   \n",
       "4            5  we got away, we got into this feeling that val...       -1   \n",
       "\n",
       "   nn_count  pr_count  vb_count  jj_count  uh_count  cd_count  num_dialog  \\\n",
       "0         2         0         0         0         0         0           0   \n",
       "1         4         0         0         0         0         0           0   \n",
       "2         6         0         0         0         0         0           0   \n",
       "3         8         0         0         0         0         0           0   \n",
       "4         3         0         0         0         0         0           0   \n",
       "\n",
       "   num_narrative  unique_vs_words  stopwords_vs_words  \n",
       "0              1         1.000000            0.625000  \n",
       "1              1         0.875000            0.687500  \n",
       "2              1         0.931034            0.758621  \n",
       "3              1         0.885714            0.685714  \n",
       "4              1         0.866667            0.466667  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting our final training data ready:\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Define the necessary dataframes to be used.\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# to be performed only for training data to improve quality of training dataset.\n",
    "def casefold_and_remove_duplicates_and_contradicts(df):\n",
    "    # Store a copy of the original df to be used later.\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    # first convert our Text column from object to string\n",
    "    df['Text'] = df['Text'].astype('string')\n",
    "    df_original['Text'] = df_original['Text'].astype('string')\n",
    "    # Do case folding by using map & lower function\n",
    "    df['Text']=df['Text'].map(str.lower)\n",
    "    df_original['Text']=df_original['Text'].map(str.lower)\n",
    "    \n",
    "    ################################### START TO HANDLE DATA W/ CONTRADICTING LABEL ##########################\n",
    "    \n",
    "    # Start to remove exact duplicates where text and label are the same, as it could lead to overfitting.\n",
    "    df2 = df.groupby(['Text', 'Verdict']).size().reset_index(name='counts')\n",
    "    df2 = df2.sort_values('counts',ascending=False)\n",
    "    \n",
    "    # Remove contradicting labels for the same statement\n",
    "    contradicting_label_text_df = df2.loc[df2['Text'].duplicated(),:]\n",
    "    # for all text data that we see in contradicting_label_text_df,\n",
    "    # decide how to fix the labels for such data, since there are only 7 of such occurrences\n",
    "    all_contradicting_label_text_df = pd.DataFrame()\n",
    "\n",
    "    for index,row in contradicting_label_text_df.iterrows():\n",
    "        df_to_append = df_original.loc[df_original.Text == row['Text']]\n",
    "        all_contradicting_label_text_df = pd.concat([all_contradicting_label_text_df, df_to_append])\n",
    "        \n",
    "    # pad the observations to the minority. In addition, this will also result in help in the data imbalance.\n",
    "    index_to_update_verdict = [] # store all the indexes to be updated for verdict in training data set\n",
    "    \n",
    "    for index, row in contradicting_label_text_df.iterrows():\n",
    "        # print(row['Text'])\n",
    "        # print(df.loc[df['Text'] == row['Text']].index)\n",
    "        index_to_update_verdict.extend(df_original.loc[df_original['Text'] == row['Text']].index)\n",
    "\n",
    "    # Loop through the indexes to update the verdict column for these indexes in df_original\n",
    "    for index in index_to_update_verdict:\n",
    "        df_original.loc[df_original.index == index, ['Verdict']] = 0\n",
    "    \n",
    "    ################################### END OF HANDLING DATA W/ CONTRADICTING LABEL ##########################\n",
    "    \n",
    "    ################################### START TO HANDLE DUPLICATED DATA ##########################\n",
    "    # After handling data with contradicting label, find the new df2 to remove duplicates again\n",
    "    new_df = df_original.drop_duplicates(keep='first', subset=['Text'])\n",
    "    \n",
    "    # return the preprocessed new_df\n",
    "    return new_df\n",
    "    \n",
    "    \n",
    "# Define a function to count the Part of Speech categories in each sentence, and apply it to a dataframe\n",
    "def pos_count(sent):\n",
    "    nn_count = 0   #Noun\n",
    "    pr_count = 0   #Pronoun\n",
    "    vb_count = 0   #Verb\n",
    "    jj_count = 0   #Adjective\n",
    "    uh_count = 0   #Interjection\n",
    "    cd_count = 0   #Numerics\n",
    "    \n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    for token in sent:\n",
    "            if token[1] in ['NN','NNP','NNS']:\n",
    "                nn_count += 1\n",
    "    if token[1] in ['PRP','PRP$']:\n",
    "                pr_count += 1\n",
    "    if token[1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                vb_count += 1\n",
    "    if token[1] in ['JJ','JJR','JJS']:\n",
    "                jj_count += 1\n",
    "    if token[1] in ['UH']:\n",
    "                uh_count += 1\n",
    "    if token[1] in ['CD']:\n",
    "                cd_count += 1\n",
    "    return pd.Series([nn_count, pr_count, vb_count, jj_count, uh_count, cd_count])\n",
    "\n",
    "def dialog_parser(text):\n",
    "    \n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    \n",
    "    # let's set up some lists to hold our pieces of narrative and dialog\n",
    "    parsed_dialog = []\n",
    "    parsed_narrative = []\n",
    "    \n",
    "    # and this list will be a bucket for the text we're currently exploring\n",
    "    current = []\n",
    "    # now let's set up values that will help us loop through the text\n",
    "    length = len(tokenized)\n",
    "    found_q = False\n",
    "    counter = 0\n",
    "    quote_open, quote_close = '``', \"''\"\n",
    "    # now we'll start our loop saying that as long as our sentence is...\n",
    "    while counter < length:\n",
    "        word = tokenized[counter]\n",
    "        # until we find a quotation mark, we're working with narrative\n",
    "        if quote_open not in word and quote_close not in word:\n",
    "            current.append(word)\n",
    "        # here's what we do when we find a closed quote\n",
    "        else:\n",
    "            # we append the narrative we've collected & clear our our\n",
    "            # current variable\n",
    "            parsed_narrative.append(current)\n",
    "            current = []\n",
    "            \n",
    "            # now current is ready to hold dialog and we're working on\n",
    "            # a piece of dialog\n",
    "            current.append(word)\n",
    "            found_q = True\n",
    "            # while we're in the quote, we're going to increment the counter\n",
    "            # and append to current in this while loop\n",
    "            while found_q and counter < length-1:\n",
    "                counter += 1\n",
    "                if quote_close not in tokenized[counter]:\n",
    "                    current.append(tokenized[counter])\n",
    "                else:\n",
    "                    # if we find a closing quote, we add our dialog to the\n",
    "                    # appropriate list, clear current and flip our found_q\n",
    "                    # variable to False\n",
    "                    current.append(tokenized[counter])\n",
    "                    parsed_dialog.append(current)\n",
    "                    current = []\n",
    "                    found_q = False\n",
    "        # increment the counter to move us through the text\n",
    "        counter += 1\n",
    "    \n",
    "    if len(parsed_narrative) == 0:\n",
    "        parsed_narrative.append(current)\n",
    "    \n",
    "    mean_dialog_word_len = 0\n",
    "    \n",
    "    if len(parsed_dialog) > 0:\n",
    "        for text in parsed_dialog:\n",
    "            join_text = \" \".join(text)\n",
    "            join_text = join_text.replace('\"','')\n",
    "            join_text = join_text.replace(\"''\",\"\")\n",
    "            mean_dialog_word_len += len(join_text.split())\n",
    "        \n",
    "        mean_dialog_word_len /= float(len(parsed_dialog))\n",
    "    \n",
    "    mean_narrative_word_len = 0\n",
    "    \n",
    "    if len(parsed_narrative) > 0:\n",
    "        for text in parsed_narrative:\n",
    "            join_text = \" \".join(text)\n",
    "            join_text = join_text.replace('\"','')\n",
    "            join_text = join_text.replace(\"''\",\"\")\n",
    "            mean_narrative_word_len += len(join_text.split())\n",
    "        \n",
    "        mean_narrative_word_len /= float(len(parsed_narrative))\n",
    "    return len(parsed_dialog), len(parsed_narrative), mean_dialog_word_len, mean_narrative_word_len\n",
    "\n",
    "\n",
    "# Counting all unique words in a sentence\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "# Counting all words in a sentence\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stop_words = [w for w in word_tokens if w in sw_spacy]\n",
    "    return len(stop_words)\n",
    "\n",
    "# Define the column names to be inserted in both training and testing data and computed together \n",
    "POS_columns = ['nn_count', 'pr_count', 'vb_count', 'jj_count','uh_count', 'cd_count']\n",
    "dialog_narrative_cols = [\"num_dialog\", \"num_narrative\"]\n",
    "combined_cols = POS_columns + dialog_narrative_cols\n",
    "\n",
    "# final column names for test & train data set\n",
    "combined_cols_v2 = combined_cols + ['unique_vs_words','stopwords_vs_words']\n",
    "\n",
    "\n",
    "# apply on training dataset our feature engineering\n",
    "def open_df_and_return_df_POS_dialognarrative_ratios(df):\n",
    "    df_copy = df.copy() # make a copy so that we do not make edits on the current version.\n",
    "    # Feature: Apply POS tagging, and dialog / narrative parsing.\n",
    "    df_copy[POS_columns] = df_copy['Text'].apply(pos_count)\n",
    "    df_copy[dialog_narrative_cols[0]] = df_copy['Text'].apply(lambda x: dialog_parser(x)[0])\n",
    "    df_copy[dialog_narrative_cols[1]] = df_copy['Text'].apply(lambda x: dialog_parser(x)[1])\n",
    "    \n",
    "    # apply word count/stopword count ratios on our dataframe\n",
    "    df_copy['word_count'] = df_copy['Text'].apply(lambda x: count_words(x))\n",
    "    df_copy['unique_word_count'] = df_copy['Text'].apply(lambda x: count_unique_words(x))\n",
    "    df_copy['stopword_count'] = df_copy['Text'].apply(lambda x: count_stopwords(x))\n",
    "    \n",
    "    # Feature: unique words ratio in sentence\n",
    "    df_copy['unique_vs_words'] = df_copy['unique_word_count']/df_copy['word_count']\n",
    "    # Feature: stopwords ratio in sentence\n",
    "    df_copy['stopwords_vs_words'] = df_copy['stopword_count']/df_copy['word_count']\n",
    "    \n",
    "    # Drop unnecessary columns inplace\n",
    "    df_copy.drop(columns=['word_count','unique_word_count','stopword_count'], inplace=True)\n",
    "    # df_copy.drop(columns=['word_count','unique_word_count','stopword_count','char_count'], inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# Casefold & remove duplicates first:\n",
    "train_df_v1 = casefold_and_remove_duplicates_and_contradicts(train_df)\n",
    "train_df_final = open_df_and_return_df_POS_dialognarrative_ratios(train_df_v1)\n",
    "train_df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0209f",
   "metadata": {},
   "source": [
    "# Handle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a829332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "      <th>num_dialog</th>\n",
       "      <th>num_narrative</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>you know, i saw a movie - \"crocodile dundee.\"</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we're consuming 50 percent of the world's coca...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>that answer was about as clear as boston harbor.</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>let me help the governor.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>we've run up more debt in the last eight years...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  nn_count  \\\n",
       "0            1      you know, i saw a movie - \"crocodile dundee.\"         2   \n",
       "1            2  we're consuming 50 percent of the world's coca...         3   \n",
       "2            3   that answer was about as clear as boston harbor.         3   \n",
       "3            4                          let me help the governor.         1   \n",
       "4            5  we've run up more debt in the last eight years...         6   \n",
       "\n",
       "   pr_count  vb_count  jj_count  uh_count  cd_count  num_dialog  \\\n",
       "0         0         0         0         0         0           1   \n",
       "1         0         0         0         0         0           0   \n",
       "2         0         0         0         0         0           0   \n",
       "3         0         0         0         0         0           0   \n",
       "4         0         0         0         0         0           0   \n",
       "\n",
       "   num_narrative  unique_vs_words  stopwords_vs_words  \n",
       "0              1         1.000000            0.333333  \n",
       "1              1         1.000000            0.625000  \n",
       "2              1         0.888889            0.555556  \n",
       "3              1         1.000000            0.400000  \n",
       "4              1         0.954545            0.636364  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the following functions to find our ratios of uniquewords/total words and stopwords/total words\n",
    "\n",
    "# apply on test dataset\n",
    "def open_testdf_and_return_testdf_casefold_POS_dialognarrative_ratios(df):\n",
    "#     df = pd.read_csv('test.csv')\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Text'] = df_copy['Text'].astype('string')\n",
    "    df_copy['Text'] = df_copy['Text'].map(str.lower)\n",
    "    # Apply sentence_subjectivity function to our test text data and get the scores accordingly.\n",
    "    df_copy[POS_columns] = df_copy['Text'].apply(pos_count)\n",
    "    df_copy[dialog_narrative_cols[0]] = df_copy['Text'].apply(lambda x: dialog_parser(x)[0])\n",
    "    df_copy[dialog_narrative_cols[1]] = df_copy['Text'].apply(lambda x: dialog_parser(x)[1])\n",
    "    # df_copy['char_count'] = df_copy['Text'].apply(lambda x: count_chars(x))\n",
    "    df_copy['word_count'] = df_copy['Text'].apply(lambda x: count_words(x))\n",
    "    df_copy['unique_word_count'] = df_copy['Text'].apply(lambda x: count_unique_words(x))\n",
    "    df_copy['stopword_count'] = df_copy['Text'].apply(lambda x: count_stopwords(x))\n",
    "    df_copy['unique_vs_words'] = df_copy['unique_word_count']/df_copy['word_count']\n",
    "    df_copy['stopwords_vs_words'] = df_copy['stopword_count']/df_copy['word_count']\n",
    "    # df_copy['avg_wordlength'] = df_copy['char_count']/df_copy['word_count']\n",
    "    \n",
    "    # Drop unnecessary columns inplace\n",
    "    df_copy.drop(columns=['word_count','unique_word_count','stopword_count'], inplace=True)\n",
    "    # df_copy.drop(columns=['word_count','unique_word_count','stopword_count','char_count'], inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "# TESTING\n",
    "test_df_POS_dialog_narrative_ratios = open_testdf_and_return_testdf_casefold_POS_dialognarrative_ratios(test_df)\n",
    "test_df_POS_dialog_narrative_ratios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4a4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to take a training dataframe and output the sentences and the corresponding labels\n",
    "def get_sentences_and_label(traindf):\n",
    "    sentences = traindf['Text']\n",
    "    labels = traindf['Verdict']\n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4f8a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "      <th>num_dialog</th>\n",
       "      <th>num_narrative</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i think we've seen a deterioration of values.</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>i think for a while as a nation we condoned th...</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>for a while, as i recall, it even seems to me ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>so we've seen a deterioration in values, and o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>we got away, we got into this feeling that val...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  Verdict  \\\n",
       "0            1      i think we've seen a deterioration of values.       -1   \n",
       "1            2  i think for a while as a nation we condoned th...       -1   \n",
       "2            3  for a while, as i recall, it even seems to me ...       -1   \n",
       "3            4  so we've seen a deterioration in values, and o...       -1   \n",
       "4            5  we got away, we got into this feeling that val...       -1   \n",
       "\n",
       "   nn_count  pr_count  vb_count  jj_count  uh_count  cd_count  num_dialog  \\\n",
       "0         2         0         0         0         0         0           0   \n",
       "1         4         0         0         0         0         0           0   \n",
       "2         6         0         0         0         0         0           0   \n",
       "3         8         0         0         0         0         0           0   \n",
       "4         3         0         0         0         0         0           0   \n",
       "\n",
       "   num_narrative  unique_vs_words  stopwords_vs_words  \n",
       "0              1         1.000000            0.625000  \n",
       "1              1         0.875000            0.687500  \n",
       "2              1         0.931034            0.758621  \n",
       "3              1         0.885714            0.685714  \n",
       "4              1         0.866667            0.466667  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3e4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nn_count', 'pr_count', 'vb_count', 'jj_count', 'uh_count', 'cd_count',\n",
      "       'num_dialog', 'num_narrative', 'unique_vs_words', 'stopwords_vs_words'],\n",
      "      dtype='object')\n",
      "Index(['nn_count', 'pr_count', 'vb_count', 'jj_count', 'uh_count', 'cd_count',\n",
      "       'num_dialog', 'num_narrative', 'unique_vs_words', 'stopwords_vs_words'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# specify train_df_final_v2 which drops sentence id and text column\n",
    "train_df_final_v2 = train_df_final.drop(columns=['Sentence_id','Text','Verdict'])\n",
    "\n",
    "print(train_df_final_v2.columns)\n",
    "\n",
    "# specify test_df_POS_dialog_narrative_ratios\n",
    "test_df_final_v2 = test_df_POS_dialog_narrative_ratios.drop(columns=['Sentence_id','Text'])\n",
    "print(test_df_final_v2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb46df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>pr_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>uh_count</th>\n",
       "      <th>cd_count</th>\n",
       "      <th>num_dialog</th>\n",
       "      <th>num_narrative</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>you know, i saw a movie - \"crocodile dundee.\"</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we're consuming 50 percent of the world's coca...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>that answer was about as clear as boston harbor.</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>let me help the governor.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>we've run up more debt in the last eight years...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text  nn_count  \\\n",
       "0            1      you know, i saw a movie - \"crocodile dundee.\"         2   \n",
       "1            2  we're consuming 50 percent of the world's coca...         3   \n",
       "2            3   that answer was about as clear as boston harbor.         3   \n",
       "3            4                          let me help the governor.         1   \n",
       "4            5  we've run up more debt in the last eight years...         6   \n",
       "\n",
       "   pr_count  vb_count  jj_count  uh_count  cd_count  num_dialog  \\\n",
       "0         0         0         0         0         0           1   \n",
       "1         0         0         0         0         0           0   \n",
       "2         0         0         0         0         0           0   \n",
       "3         0         0         0         0         0           0   \n",
       "4         0         0         0         0         0           0   \n",
       "\n",
       "   num_narrative  unique_vs_words  stopwords_vs_words  \n",
       "0              1         1.000000            0.333333  \n",
       "1              1         1.000000            0.625000  \n",
       "2              1         0.888889            0.555556  \n",
       "3              1         1.000000            0.400000  \n",
       "4              1         0.954545            0.636364  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_POS_dialog_narrative_ratios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385bd43",
   "metadata": {},
   "source": [
    "## Based on cross-validated results and top 4 model results, the best performing model and hyper parameters that returned the highest test f1_score on KAGGLE was:\n",
    "\n",
    "Logistic Regression (tfidf, no resampling)\t{'columntransform__vect__ngram_range': (1, 1), 'columntransform__vect__norm': 'l2', 'lr__C': 1.0000000000000002, 'lr__penalty': 'l2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = get_sentences_and_label(train_df_final)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1)) # best hyperparameter setting\n",
    "\n",
    "# Train model on full training dataset and make an array\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(sentences).toarray() # perform fit_transform\n",
    "\n",
    "# Get sentences from test data and transform with tfidf and make an array\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['Text']).toarray()\n",
    "\n",
    "# Convert the above list to a DataFrame\n",
    "train_tfidf = pd.DataFrame(X_train_tfidf)\n",
    "train_tfidf.reset_index(inplace=True, drop=True)\n",
    "test_tfidf = pd.DataFrame(X_test_tfidf)\n",
    "test_tfidf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_df_final_v2.reset_index(inplace=True,drop=True)\n",
    "test_df_final_v2.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_POS_dialog_narrative_ratios_feature = new_df_POS_dialog_narrative_ratios[combined_cols_v2]\n",
    "# train_POS_dialog_narrative_ratios_feature.reset_index(inplace=True, drop=True)\n",
    "# test_POS_dialog_narrative_ratios_feature = test_df_POS_dialog_narrative_ratios[combined_cols_v2]\n",
    "# test_POS_dialog_narrative_ratios_feature.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Merge the tfidf feature and other feature engineered variables using pd.merge\n",
    "X_train = pd.merge(train_tfidf, train_df_final_v2, left_index=True, right_index=True)\n",
    "X_test = pd.merge(test_tfidf, test_df_final_v2,left_index=True, right_index=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# MinMaxScaler for normalization for combined features of tfidf and subjectivity data.\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "# Perform normalization\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression classifier with data\n",
    "LR_classifier = LogisticRegression(random_state=randomseed, solver='liblinear',C=1,penalty='l2').fit(X_train_scaled, labels)\n",
    "\n",
    "# Do predictions on X_test_scaled\n",
    "y_pred_full = LR_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Generate the csv file to submit to kaggle\n",
    "generate_result(test_df_POS_dialog_narrative_ratios.drop(columns=combined_cols_v2), y_pred_full, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dba56",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
